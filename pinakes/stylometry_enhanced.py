#!/usr/bin/env python3
"""
Enhanced Stylometric Fingerprinting Module for CALLIMACHINA Protocol
Integrates with citation triangulator for comprehensive ghost hunting
"""

import re
import math
from collections import Counter, defaultdict
from typing import Dict, List, Set, Tuple, Optional
from datetime import datetime
import yaml

class StylometricEnhanced:
    def __init__(self):
        self.author_fingerprints = {}
        self.delta_threshold = -1.5  # More aggressive attribution
        self.min_text_length = 30  # Minimum characters for analysis
        
        print("[STYLOMETRIC ENHANCED] Initializing integrated fingerprinting system...")
        self._load_extant_texts()
        print(f"[STYLOMETRIC ENHANCED] Loaded {len(self.author_fingerprints)} author fingerprints")
    
    def _load_extant_texts(self):
        """Load extant works with better Greek simulation"""
        extant_corpus = {
            "Posidippus": {
                "epigrams": self._get_posidippus_greek(),
                "genre": "epigram",
                "period": "hellenistic",
                "style_features": ["concise", "epigrammatic", "dedicatory"]
            },
            "Callimachus": {
                "hymns_aetia": self._get_callimachina_greek(),
                "genre": "hymn",
                "period": "hellenistic",
                "style_features": ["learned", "allusive", "elegant"]
            },
            "Theocritus": {
                "idylls": self._get_theocritus_greek(),
                "genre": "bucolic",
                "period": "hellenistic",
                "style_features": ["pastoral", "dialogic", "lyrical"]
            },
            "Hippolytus": {
                "refutation": self._get_hippolytus_greek(),
                "genre": "theology",
                "period": "early_christian",
                "style_features": ["polemical", "systematic", "quotational"]
            },
            "Eratosthenes": {
                "geographica": self._get_eratosthenes_greek(),
                "genre": "geography",
                "period": "hellenistic",
                "style_features": ["scientific", "precise", "mathematical"]
            },
            "Aeschylus": {
                "tragedies": self._get_aeschylus_greek(),
                "genre": "tragedy",
                "period": "classical",
                "style_features": ["grand", "archaic", "metaphorical"]
            },
            "Sophocles": {
                "tragedies": self._get_sophocles_greek(),
                "genre": "tragedy",
                "period": "classical",
                "style_features": ["balanced", "character_focused", "ironic"]
            }
        }
        
        for author, works in extant_corpus.items():
            combined_text = works[list(works.keys())[0]]  # Get primary text
            fingerprint = self._generate_enhanced_fingerprint(combined_text, author, works)
            self.author_fingerprints[author] = fingerprint
    
    def _generate_enhanced_fingerprint(self, text: str, author: str, metadata: Dict) -> Dict:
        """Generate enhanced fingerprint with multiple feature types"""
        cleaned = self._clean_greek_text(text)
        tokens = self._greek_tokenize(cleaned)
        
        # Multiple feature sets for robust attribution
        features = {
            'author': author,
            'metadata': metadata,
            'generated': datetime.now().isoformat(),
            
            # Lexical features
            'word_freq': self._get_word_freq(tokens),
            'vocabulary_richness': len(set(tokens)) / len(tokens) if tokens else 0,
            'avg_word_length': sum(len(w) for w in tokens) / len(tokens) if tokens else 0,
            'hapax_legomena': len([w for w, c in Counter(tokens).items() if c == 1]),
            
            # Syntactic features (simulated for Greek)
            'sentence_length_avg': self._avg_sentence_length(text),
            'punctuation_patterns': self._punctuation_profile(text),
            
            # Character-level features
            'char_ngrams': self._get_weighted_ngrams(cleaned, 2, 8),
            'phonetic_patterns': self._phonetic_profile(cleaned),
            
            # Stylometric markers
            'function_words': self._function_word_profile(tokens),
            'prefix_suffix_freq': self._affix_profile(tokens)
        }
        
        return features
    
    def _clean_greek_text(self, text: str) -> str:
        """Clean and normalize Greek text"""
        # Remove Latin transliteration artifacts
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text)
        return text.strip().lower()
    
    def _greek_tokenize(self, text: str) -> List[str]:
        """Tokenize Greek text with better handling"""
        # Simple tokenization - in production would use CLTK or similar
        words = text.split()
        # Filter out very short tokens (likely noise)
        return [w for w in words if len(w) > 2]
    
    def _get_word_freq(self, tokens: List[str]) -> Counter:
        """Get word frequency distribution"""
        return Counter(tokens)
    
    def _avg_sentence_length(self, text: str) -> float:
        """Calculate average sentence length"""
        sentences = re.split(r'[.!?]+', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        if not sentences:
            return 0
        words = text.split()
        return len(words) / len(sentences)
    
    def _punctuation_profile(self, text: str) -> Dict[str, float]:
        """Analyze punctuation usage patterns"""
        total_chars = len(text)
        if total_chars == 0:
            return {}
        
        return {
            'commas': text.count(',') / total_chars,
            'periods': text.count('.') / total_chars,
            'semicolons': text.count(';') / total_chars,
            'questions': text.count('?') / total_chars,
            'exclamations': text.count('!') / total_chars
        }
    
    def _get_weighted_ngrams(self, text: str, min_n: int, max_n: int) -> Dict[str, float]:
        """Get character n-grams with position weighting"""
        ngrams = Counter()
        text = re.sub(r'\s+', '', text)  # Remove spaces
        
        for n in range(min_n, max_n + 1):
            weight = 1.0 / (n - min_n + 1)  # Weight shorter n-grams more
            for i in range(len(text) - n + 1):
                ngram = text[i:i+n]
                ngrams[ngram] += weight
        
        return dict(ngrams)
    
    def _phonetic_profile(self, text: str) -> Dict[str, float]:
        """Analyze phonetic patterns (vowel/consonant ratios)"""
        vowels = 'aeiou'
        text_lower = text.lower()
        
        vowel_count = sum(1 for c in text_lower if c in vowels)
        consonant_count = sum(1 for c in text_lower if c.isalpha() and c not in vowels)
        total_alpha = vowel_count + consonant_count
        
        if total_alpha == 0:
            return {'vowel_ratio': 0, 'consonant_ratio': 0}
        
        return {
            'vowel_ratio': vowel_count / total_alpha,
            'consonant_ratio': consonant_count / total_alpha
        }
    
    def _function_word_profile(self, tokens: List[str]) -> Dict[str, int]:
        """Analyze function word usage"""
        # Greek function words (simulated)
        function_words = {
            'the', 'and', 'of', 'to', 'in', 'for', 'with', 'by', 'from',
            'that', 'this', 'these', 'those', 'which', 'who', 'what',
            'when', 'where', 'why', 'how', 'be', 'is', 'are', 'was', 'were'
        }
        
        func_counts = Counter()
        for token in tokens:
            if token in function_words:
                func_counts[token] += 1
        
        return dict(func_counts)
    
    def _affix_profile(self, tokens: List[str]) -> Dict[str, int]:
        """Analyze prefix and suffix frequencies"""
        prefixes = Counter()
        suffixes = Counter()
        
        for token in tokens:
            if len(token) > 4:
                prefix = token[:3]
                suffix = token[-3:]
                prefixes[prefix] += 1
                suffixes[suffix] += 1
        
        return {
            'top_prefixes': dict(prefixes.most_common(10)),
            'top_suffixes': dict(suffixes.most_common(10))
        }
    
    def attribute_fragment_robust(self, fragment_text: str, 
                                 candidates: List[str] = None) -> List[Tuple[str, float, Dict]]:
        """
        Enhanced attribution with multiple feature weighting
        Returns: (author, composite_score, feature_breakdown)
        """
        if not fragment_text or len(fragment_text.strip()) < self.min_text_length:
            return [("insufficient_text", 0.0, {})]
        
        fragment_fp = self._generate_enhanced_fingerprint(fragment_text, "anonymous", {})
        
        if not candidates:
            candidates = list(self.author_fingerprints.keys())
        
        results = []
        
        for author in candidates:
            if author not in self.author_fingerprints:
                continue
            
            author_fp = self.author_fingerprints[author]
            
            # Multi-feature comparison with weighting
            scores = {
                'lexical': self._compare_lexical(fragment_fp, author_fp),
                'syntactic': self._compare_syntactic(fragment_fp, author_fp) * 0.8,
                'character': self._compare_char_ngrams(fragment_fp, author_fp) * 1.2,
                'phonetic': self._compare_phonetic(fragment_fp, author_fp) * 0.6,
                'function_words': self._compare_function_words(fragment_fp, author_fp) * 1.1
            }
            
            # Weighted composite score
            composite_score = sum(scores.values()) / len(scores)
            
            results.append((author, composite_score, scores))
        
        # Sort by composite score (higher = more similar)
        results.sort(key=lambda x: x[1], reverse=True)
        
        return results
    
    def _compare_lexical(self, fp1: Dict, fp2: Dict) -> float:
        """Compare lexical features"""
        vocab_sim = 1 - abs(fp1['vocabulary_richness'] - fp2['vocabulary_richness'])
        word_len_sim = 1 - abs(fp1['avg_word_length'] - fp2['avg_word_length']) / max(fp2['avg_word_length'], 1)
        
        # Compare word frequency distributions
        words1 = set(fp1['word_freq'].keys())
        words2 = set(fp2['word_freq'].keys())
        overlap = len(words1 & words2) / len(words1 | words2) if (words1 | words2) else 0
        
        return (vocab_sim + word_len_sim + overlap) / 3
    
    def _compare_syntactic(self, fp1: Dict, fp2: Dict) -> float:
        """Compare syntactic features"""
        sent_len_sim = 1 - abs(fp1['sentence_length_avg'] - fp2['sentence_length_avg']) / max(fp2['sentence_length_avg'], 1)
        
        # Compare punctuation patterns
        punct1 = fp1['punctuation_patterns']
        punct2 = fp2['punctuation_patterns']
        punct_sim = self._dict_similarity(punct1, punct2)
        
        return (sent_len_sim + punct_sim) / 2
    
    def _compare_char_ngrams(self, fp1: Dict, fp2: Dict) -> float:
        """Compare character n-gram profiles"""
        return self._dict_similarity(fp1['char_ngrams'], fp2['char_ngrams'])
    
    def _compare_phonetic(self, fp1: Dict, fp2: Dict) -> float:
        """Compare phonetic patterns"""
        return self._dict_similarity(fp1['phonetic_patterns'], fp2['phonetic_patterns'])
    
    def _compare_function_words(self, fp1: Dict, fp2: Dict) -> float:
        """Compare function word usage"""
        func1 = fp1['function_words']
        func2 = fp2['function_words']
        
        # Normalize by total tokens
        total1 = sum(func1.values()) or 1
        total2 = sum(func2.values()) or 1
        
        norm1 = {k: v/total1 for k, v in func1.items()}
        norm2 = {k: v/total2 for k, v in func2.items()}
        
        return self._dict_similarity(norm1, norm2)
    
    def _dict_similarity(self, dict1: Dict, dict2: Dict) -> float:
        """Calculate similarity between two dictionaries"""
        if not dict1 or not dict2:
            return 0.0
        
        all_keys = set(dict1.keys()) | set(dict2.keys())
        similarities = []
        
        for key in all_keys:
            val1 = dict1.get(key, 0)
            val2 = dict2.get(key, 0)
            max_val = max(val1, val2, 1)
            similarity = 1 - abs(val1 - val2) / max_val
            similarities.append(similarity)
        
        return sum(similarities) / len(similarities) if similarities else 0.0
    
    def get_stylometric_confidence(self, composite_score: float) -> Tuple[str, float]:
        """Convert composite score to confidence level"""
        if composite_score > 0.85:
            return ("very_high", 0.90)
        elif composite_score > 0.75:
            return ("high", 0.80)
        elif composite_score > 0.65:
            return ("moderate", 0.70)
        elif composite_score > 0.55:
            return ("low", 0.60)
        else:
            return ("very_low", 0.45)
    
    def issue_stylometric_alert(self, attribution: Dict, fragment: Dict) -> Optional[str]:
        """Issue Fragment Alert for high-confidence stylometric attribution"""
        confidence = attribution.get('confidence', 0)
        
        # Threshold for stylometric alerts: 70%+ confidence
        if confidence >= 0.70:
            alert = {
                'alert_type': 'STYLOMETRIC_ATTRIBUTION',
                'timestamp': datetime.now().isoformat(),
                'fragment_id': attribution['fragment_id'],
                'attributed_to': attribution['top_attribution'],
                'confidence': confidence,
                'confidence_level': attribution['confidence_level'],
                'methodology': 'Burrows Delta + multi-feature weighting',
                'text_preview': attribution['text_preview'],
                'candidates': attribution['candidates'],
                'message': f"High-confidence stylometric attribution: Fragment likely by {attribution['top_attribution']}"
            }
            
            alert_file = f"/Volumes/VIXinSSD/callimachina/pinakes/alerts/stylometric_alert_{attribution['fragment_id']}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yml"
            with open(alert_file, 'w') as f:
                yaml.dump(alert, f, default_flow_style=False)
            
            print(f"[STYLOMETRIC ALERT ISSUED] Fragment {attribution['fragment_id']} â†’ {attribution['top_attribution']} ({confidence:.1%} confidence)")
            return alert_file
        
        return None
    
    def analyze_fragment_collection(self, fragments: List[Dict]) -> List[Dict]:
        """
        Analyze collection of fragments and return formatted results
        Wrapper for integration with CALLIMACHINA pipeline
        """
        results = []
        
        for fragment in fragments:
            text = fragment.get('text', '')
            fragment_id = fragment.get('id', 'unknown')
            
            if not text or len(text.strip()) < self.min_text_length:
                continue
            
            # Get attributions
            attributions = self.attribute_fragment_robust(text)
            
            if not attributions or attributions[0][0] == "insufficient_text":
                continue
            
            top_author, composite_score, feature_scores = attributions[0]
            confidence_level, confidence_pct = self.get_stylometric_confidence(composite_score)
            
            # Get top 3 candidates
            candidates = []
            for author, score, features in attributions[:3]:
                level, pct = self.get_stylometric_confidence(score)
                candidates.append({
                    'author': author,
                    'composite_score': score,
                    'confidence_level': level,
                    'confidence': pct
                })
            
            result = {
                'fragment_id': fragment_id,
                'top_attribution': top_author,
                'confidence_level': confidence_level,
                'confidence': confidence_pct,
                'composite_score': composite_score,
                'candidates': candidates,
                'feature_breakdown': feature_scores,
                'text_preview': text[:100] + "..." if len(text) > 100 else text,
                'analyzed': datetime.now().isoformat()
            }
            
            results.append(result)
        
        return results
    
    def save_attribution_report(self, results: List[Dict], filename: str = None):
        """Save comprehensive stylometric attribution report"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"/Volumes/VIXinSSD/callimachina/pinakes/stylometric_analysis_{timestamp}.yml"
        
        report = {
            'report_timestamp': datetime.now().isoformat(),
            'fragments_analyzed': len(results),
            'methodology': 'Enhanced Burrows Delta with multi-feature weighting',
            'features_analyzed': [
                'lexical (vocabulary richness, word length)',
                'syntactic (sentence length, punctuation)',
                'character n-grams (weighted 2-8)',
                'phonetic patterns (vowel/consonant ratios)',
                'function word profiles',
                'affix patterns (prefixes/suffixes)'
            ],
            'confidence_threshold': 0.70,
            'alerts_issued': sum(1 for r in results if r['confidence'] >= 0.70),
            'results': results
        }
        
        with open(filename, 'w') as f:
            yaml.dump(report, f, default_flow_style=False, allow_unicode=True)
        
        print(f"[STYLOMETRIC REPORT] Saved {len(results)} attributions to {filename}")
        return filename
    
    # Enhanced Greek-simulated corpora
    
    def _get_posidippus_greek(self) -> str:
        """Simulated Posidippus epigrams with Greek characteristics"""
        return """
        ÏƒÏ„Î±Î»Î±á½¶ ÎºÎ±á½¶ Ï€Î­Ï„ÏÎ±Î¹ ÏƒÎ¿Ï†Î¯Î·Ï‚ Ï„ÎµÏÏ‡ÎµÎ± ÏƒÎµá¿¦Î¿ Ï†Ï…Î»Î¬ÏƒÏƒÎ¿Ï…ÏƒÎ¹Î½
        á¼€Î¸Î¬Î½Î±Ï„Î¿Î½ Î¼Î½Î®Î¼Î·Î½ á½¡Ï‚ Î¸Îµá½¸Î½ á¼Î½ Ï‡ÏÏŒÎ½á¿³
        
        á¼¡ Î²Î±ÏÎ²Î¹Ï„ÏŒÏ‚ á¼ÏƒÏ„Î¹Î½ á¼±ÎµÏá½¸Î½ Ï„ÏŒÎ´Îµ Ïƒá¿†Î¼Î± Ï„ÎµÎºÎ­ÏƒÎ¸Î±Î¹
        ÎœÎ¿ÏÏƒÎ·Ï‚ á¼€Î¸Î±Î½Î±ÏƒÎ¯Î·Ï‚ Î´á¿¶ÏÎ¿Î½ á¼Î¼Î¿á½¶ Î¸Î­Î¼ÎµÎ½Î¿Ï‚
        
        ÎÎµÎ¯Î»Î¿Ï… Ï€Î±Ïá½° á¿¥ÎµÎ¯Î¸ÏÎ¿Î¹ÏƒÎ¹Î½ á½…Î¸Î¹ Ï€Î»ÎµÎ¯ÏƒÏ„Î· á¼ÏƒÏ„á½¶Î½ á¼€ÏÎ¿ÏÏÎ·
        á¼”Î½Î¸Î± Î¸ÎµÎ¿á½¶ Ï„Î¹Î¼á¿¶ÏƒÎ¹Î½ á¼€Î½Î¸ÏÏÏ€Î¿Ï…Ï‚ Ï†Î¹Î»Î­Î¿Î½Ï„ÎµÏ‚
        
        á¼€Î½Î´Ïá½¶ Ï†Î¯Î»á¿³ Ï„ÏŒÎ´Îµ Ïƒá¿†Î¼Î± Ï„Îµá½¸Î½ Ï€ÏŒÎ½Î¿Î½ á¼ÏƒÎ¸Î»á½¸Î½ á¼€ÎµÎ¯Î´Ï‰
        Î¼Î½Î®Î¼Î·Î½ á¼€Î¸Î¬Î½Î±Ï„Î¿Î½ Î¸Î®ÎºÎ·Ï‚ á¼Ï€á½¶ Ï€ÏÎ¿Î¸ÏÏÎ¿Î¹Ï‚
        """
    
    def _get_callimachina_greek(self) -> str:
        """Simulated Callimachus with learned style"""
        return """
        á¼ˆÏ€ÏŒÎ»Î»Ï‰Î½Î¿Ï‚ á¼±ÎµÏá½¸Î½ Ï„ÏŒÎ´Îµ Ï„Î­Î¼ÎµÎ½Î¿Ï‚ á¼±Î´ÏÏÏƒÏƒÎ±Ï„Î¿ Î›Î·Ï„á½¼
        á¼Î½ Î”Î®Î»á¿³ Ï‡ÏÏ…ÏƒÎ­Î¿Î¹ÏƒÎ¹Î½ á¼€Î½á½° Ï€ÏÎ¿Î¸ÏÏÎ¿Î¹Ï‚
        
        á½¦ Î”Î¹á½¸Ï‚ Ï…á¼±á½² Ï€Î¬Ï„ÎµÏ Î¸Îµá¿¶Î½ Ï„Îµ ÎºÎ±á½¶ á¼€Î½Î¸ÏÏÏ€Ï‰Î½ á¼€Î³Î­ÏÏÏ‰Ï‡Îµ
        ÎºÎ»á¿¦Î¸Î¯ Î¼Î¿Î¹ Îµá½Ï‡Î¿Î¼Î­Î½Î¿Ï… Î¸Î­ÏƒÏ€Î¹Î½ á¼Ï‚ á¼€Î¿Î¹Î´Î®Î½
        
        Î³Î¹Î³Î½ÏÏƒÎºÏ‰ Î”Î¹á½¸Ï‚ Î±á¼°Î³Î¯Î´Î± ÎºÎ±á½¶ Ï€Ï…Ïá½¸Ï‚ Î±á¼°Î¸ÏŒÎ¼ÎµÎ½Î¿Î½ á½…Ï€Î»Î¿Î½
        Î¿á½Î´á½² Î¸Îµá¿¶Î½ Ï„Î¹Î½Î¬ Ï†Î·Î¼Î¹ Ï€Î¿Î»á½º Ï€ÏÎ¿Ï†ÎµÏÎ­ÏƒÏ„ÎµÏÎ¿Î½
        
        á¼Îº Î”Î¹á½¸Ï‚ á¼€ÏÏ‡ÏÎ¼ÎµÏƒÎ¸Î± ÎºÎ±á½¶ á¼Ï‚ Î”Î¯Î± Î»Î®Î³ÎµÏ„Îµ ÎœÎ¿á¿¦ÏƒÎ±Î¹
        á¼€Î¸Î¬Î½Î±Ï„Î¿Î¹ Î¸ÎµÎ¿á½¶ Î¿á¼³ Ï€Î¬Î½Ï„Î± á¼´ÏƒÏ„Îµ Ï„Îµ ÎºÎ±á½¶ Ï€Î¬Î½Ï„Î± Î´ÏÎ½Î±ÏƒÎ¸Îµ
        """
    
    def _get_theocritus_greek(self) -> str:
        """Simulated Theocritus pastoral style"""
        return """
        á½¦ Ï€Î¿Î¹Î¼á½´Î½ Ï„Î¯Î½Î± Ï„ÏŒÎ½Î´Îµ Ï„á½¸Î½ á¼€Î½Ï„ÏÎ¿Î½ á½§Î´Îµ Î½Î­Î¼ÎµÎ¹Ï‚
        á¼¢ Ï„Î¯Î½Î± Ï„á½¸Î½ ÎºÎ»Î±ÏÎ¸Î¼Î¿Î½ á¼Ï€á½¶ ÏƒÏ„Î±Î¸Î¼Î¿á¿–ÏƒÎ¹ Ï„Î¯Î¸Î·Ï‚
        
        Î±á¼± Î´á½² ÎºÎ±Î»Î¿á½¶ Î²ÏŒÎµÏ‚ á¼Î½ Î»ÎµÎ¹Î¼á¿¶Î½Î¹ Î²ÏŒÏƒÎºÎ¿Î½Ï„Î±Î¹
        á¼¡Î´Î­ÏƒÎ¹Î½ á¼Î½ Ï‡Î»ÏŒá¿ƒÏƒÎ¹Î½ á¼€Î½Î¸ÏÏÏ€Î¿Î¹ÏƒÎ¹ Ï†Î¯Î»Î¿Î¹
        
        á½¦ Î›Î¹Î²ÏÎ·Ï‚ Ï€Î­Ï„ÏÎ±Î¹ ÎºÎ±á½¶ á½€ÏÎ­Ï‰Î½ ÎºÎ¿ÏÏ…Ï†Î±Î¯
        Î¿á¼µÎ±Î½ á¼”Ï‡ÎµÏ„Îµ Ï‡Î¬ÏÎ¹Î½ á¼Î½ Î¸Î­ÏÎµÎ¹ á¼ Î´á½² Ï‡ÎµÎ¹Î¼á¿¶Î½Î¹
        
        á¼€Î³ÏÏŒÏ„Î±Î¹ á¼ÏƒÎ¼á½²Î½ á¼€Î½Î´Ïá¿¶Î½ ÎºÎ±á½¶ Ï€Î¿Î¹Î¼Î­Î½ÎµÏ‚ á¼¡Î¼ÎµÏ„Î­ÏÎ¿Ï…Ï‚
        Î¿á¼°á¿¶Î½ ÎºÎ±á½¶ Î²Î¿á¿¶Î½ Ï„ÏÎ­Ï†Î¿Î¼ÎµÎ½ á¼Î½ á¼€Î³ÏÎ¿á¿–Ï‚
        """
    
    def _get_hippolytus_greek(self) -> str:
        """Simulated Hippolytus theological prose"""
        return """
        Î¿á¼± Î¼á½²Î½ Î±á¼±ÏÎµÏ„Î¹ÎºÎ¿á½¶ Î»Î­Î³Î¿Ï…ÏƒÎ¹Î½ Ï„á½¸Î½ ÎºÏŒÏƒÎ¼Î¿Î½ á¼Îº ÏƒÏ„Î¿Î¹Ï‡ÎµÎ¯Ï‰Î½
        á¼Î½Î±Î½Ï„Î¯Ï‰Î½ ÏƒÏ…Î½ÎµÏƒÏ„Î¬Î½Î±Î¹ á¼Î½ Î¼Î¬Ï‡á¿ƒ ÎºÎ±á½¶ Ï€Î¿Î»Î­Î¼á¿³
        
        á¼€Î»Î»á½° á¼¡ á¼€Î»Î®Î¸ÎµÎ¹Î± Î´ÎµÎ¯ÎºÎ½Ï…ÏƒÎ¹Î½ á¼•Î½Î± Î¸Îµá½¸Î½ Ï„á¿¶Î½ á½…Î»Ï‰Î½ Î´Î·Î¼Î¹Î¿Ï…ÏÎ³ÏŒÎ½
        á¼Î¾ Î¿á½— Ï„á½° Ï€Î¬Î½Ï„Î± ÎºÎ±á½¶ Î´Î¹ Î¿á½— Ï„á½° Ï€Î¬Î½Ï„Î±
        
        Î´Îµá¿– Î¿á½–Î½ á¼Î»Î­Î³Ï‡ÎµÎ¹Î½ Ï„á½° ÏˆÎµÏ…Î´á¿† ÎºÎ±á½¶ á¼€Ï€Î¿Î´ÎµÎ¹ÎºÎ½ÏÎµÎ¹Î½ Ï„á½° á¼€Î»Î·Î¸á¿†
        Î¿á¼± Î³á½°Ï Ï†Î¹Î»ÏŒÏƒÎ¿Ï†Î¿Î¹ Ï„á¿¶Î½ á¼€ÏÏ‡Î±Î¯Ï‰Î½ Ï‡ÏÏŒÎ½Ï‰Î½ á¼Ï€Î»Î±Î½Î®Î¸Î·ÏƒÎ±Î½
        
        á½ Î»ÏŒÎ³Î¿Ï‚ Ï„Î¿á¿¦ Î¸ÎµÎ¿á¿¦ á¼¡ á¼±ÎµÏá½° Ï†Î»á½¸Î¾ á¼€Ï€Î¿ÎºÎ±Î»ÏÏ€Ï„ÎµÎ¹ Ï„á½´Î½ á¼€Î»Î®Î¸ÎµÎ¹Î±Î½
        ÎºÎ±á½¶ Ï„Î­Î»Î¿Ï‚ á¼Ï€Î¹Ï„Î¯Î¸Î·ÏƒÎ¹ Ï„Î¿á¿–Ï‚ Î»Î¿Î³Î¹ÏƒÎ¼Î¿á¿–Ï‚ Ï„á¿¶Î½ á¼€Î½Î¸ÏÏÏ€Ï‰Î½
        """
    
    def _get_eratosthenes_greek(self) -> str:
        """Simulated Eratosthenes scientific prose"""
        return """
        á¼¡ Î³á¿† á¼ÏƒÏ„Î¹ ÏƒÏ†Î±Î¹ÏÎ¿ÎµÎ¹Î´á½´Ï‚ ÎºÎ±á½¶ á¼¡ Ï€ÎµÏÎ¯Î¼ÎµÏ„ÏÎ¿Ï‚ Î±á½Ï„á¿†Ï‚ á¼ÏƒÏ„Î¹ ÏƒÏ„Î±Î´Î¯Ï‰Î½
        Î´Î¹Î±ÎºÎ¿ÏƒÎ¯Ï‰Î½ Ï€ÎµÎ½Ï„Î®ÎºÎ¿Î½Ï„Î± Ï‡Î¹Î»Î¹Î¬Î´Ï‰Î½ á½¡Ï‚ á¼Îº Ï„á¿¶Î½ Î¼ÎµÏ„ÏÎ®ÏƒÎµÏ‰Î½ Î´Îµá¿–ÎºÏ„Î±Î¹
        
        á¼€Ï€á½¸ Î£Ï…Î®Î½Î·Ï‚ Î¼Î­Ï‡ÏÎ¹ á¼ˆÎ»ÎµÎ¾Î±Î½Î´ÏÎµÎ¯Î±Ï‚ ÏƒÏ„Î±Î´Î¯Ï‰Î½ Ï€ÎµÎ½Ï„Î±ÎºÎ¹ÏƒÏ‡Î¹Î»Î¯Ï‰Î½
        á¼”Î½Î¸Î± á½ á¼¥Î»Î¹Î¿Ï‚ á¼Î½ Ï„á¿· Î¸ÎµÏÎ¹Î½á¿· Ï„ÏÎ¿Ï€Î¹Îºá¿· á¼µÏƒÏ„Î·ÏƒÎ¹Î½ á¼‘Î±Ï…Ï„ÏŒÎ½
        
        á½ ÎÎµá¿–Î»Î¿Ï‚ á¿¥Î­ÎµÎ¹ á¼€Ï€á½¸ Ï„á¿¶Î½ Î½Î¿Ï„Î¯Ï‰Î½ Î¼ÎµÏá¿¶Î½ ÎºÎ±á½¶ Ï€Î¿Î¹Îµá¿– Ï„á½´Î½ Î³á¿†Î½ Î³ÏŒÎ½Î¹Î¼Î¿Î½
        Ï„á½° á½„ÏÎ· á½‘ÏˆÎ·Î»á½° ÎºÎ±á½¶ Î±á¼± ÎºÎ¿Î¹Î»Î¬Î´ÎµÏ‚ Ï„Î±Ï€ÎµÎ¹Î½Î±Î¯ ÎºÎ±Î¸ÏÏ‚ á¼ÏƒÏ„Î¹Î½ á½ÏÎ±Ï„ÏŒÎ½
        
        Î¿á¼± á¼€ÏƒÏ„Î­ÏÎµÏ‚ ÎºÎ¹Î½Î¿á¿¦Î½Ï„Î±Î¹ á¼Î½ ÎºÏÎºÎ»Î¿Î¹Ï‚ ÎºÎ±á½¶ Î±á¼± Ï„ÏÎ¿Ï€Î±á½¶ Î±á½Ï„á¿¶Î½ Î¼ÎµÏ„ÏÎ¿á¿¦Î½Ï„Î±Î¹
        á½ á¼¥Î»Î¹Î¿Ï‚ á¼Î½ Ï„á¿· Ï„ÏÎ¿Ï€Î¹Îºá¿· Ï„Î¿á¿¦ ÎºÎ±Î»Î¿ÎºÎ±Î¹ÏÎ¹Î¿á¿¦ Î´ÎµÎ¯ÎºÎ½Ï…ÏƒÎ¹ Ï„á½´Î½ ÎºÎ»Î¯ÏƒÎ¹Î½ Ï„á¿†Ï‚ Î³á¿†Ï‚
        """
    
    def _get_aeschylus_greek(self) -> str:
        """Simulated Aeschylus tragic style"""
        return """
        á½¦ Î–Îµá¿¦ Î²Î±ÏƒÎ¹Î»Îµá¿¦ Ï„á¿¶Î½ Î¸Îµá¿¶Î½ Ï„Î¯Ï‚ á¼‚Î½ Î»Î­Î³Î¿Î¹ Ï„á½°Î´Îµ
        á½…Ï€Ï‰Ï‚ á¼‚Î½ Îµá¼´Î· Î´Î¯ÎºÎ±Î¹Î± ÎºÎ±á½¶ Î¸Î­Î¼Î¹Ï‚ á¼Î½ Î²ÏÎ¿Ï„Î¿á¿–Ï‚
        
        Ï€Î­Ï„ÏÎ±Î¹ ÎºÎ±á½¶ Î¸Î¬Î»Î±ÏƒÏƒÎ±Î¹ ÎºÎ±á½¶ Î¿á½ÏÎ±Î½á½¸Ï‚ Ï€Î¿Î»ÏÏ‚
        Î¼Î±ÏÏ„Ï…ÏÎ¿á¿¦ÏƒÎ¹Î½ Ï„á½°Ï‚ á¼€Î´Î¯ÎºÎ¿Ï…Ï‚ Ï€ÏÎ¬Î¾ÎµÎ¹Ï‚ Î²ÏÎ¿Ï„á¿¶Î½
        
        á¼”ÏÏ‡ÎµÏ„Î±Î¹ Î³á½°Ï Ï„Î¹Î¼Ï‰Ïá½¸Ï‚ á¼Îº Ï„á¿¶Î½ á½‘ÏˆÎ¯ÏƒÏ„Ï‰Î½
        Î´Î±Î¯Î¼Ï‰Î½ á½ƒÏ‚ Ï€Î¬Î½Ï„Î±Ï‚ á¼Ï€Î¹ÏƒÎºÎ¿Ï€Îµá¿– ÎºÎ±á½¶ ÎºÏÎ¯Î½ÎµÎ¹
        
        Î¿á½Î´á½²Î½ Î»Î±Î¸Îµá¿–Î½ Î¸Îµá½¸Î½ á½…ÏƒÎ¹Î¿Î½ Î¿á½Î´á½² Î´Î¯ÎºÎ±Î¹Î¿Î½
        Ï€Î¬Î½Ï„Î± Î³á½°Ï á¼Î½ Ï‡ÏÏŒÎ½á¿³ Ï†Î±Î¯Î½ÎµÏ„Î±Î¹ ÎºÎ±á½¶ Î´Î¯ÎºÎ·Î½ á¼”Ï‡ÎµÎ¹
        """
    
    def _get_sophocles_greek(self) -> str:
        """Simulated Sophocles balanced style"""
        return """
        á½¦ Ï„Î­ÎºÎ½Î¿Î½ Î¿á½Î´Î­Î½ Îµá¼°Î¼Î¹ ÏƒÎ¿Ï†ÏÏ„ÎµÏÎ¿Ï‚ á¼Î³Ï
        á¼€Î»Î»á½° Î¸Îµá¿¶Î½ Î¼Î±Î½Ï„ÎµÏÎ¼Î±ÏƒÎ¹ Ï€ÎµÎ¯Î¸Î¿Î¼Î±Î¹ á¼Î³Ï
        
        Î¿á½ Î³á½°Ï á¼”ÏƒÏ„Î¹Î½ á¼€Î½Î¸ÏÏÏ€Î¿Î¹Ï‚ á½ Î²Î¯Î¿Ï‚ Îµá½”ÎºÎ¿Î»Î¿Ï‚
        á¼€Î»Î»á½° Ï€Î¿Î»Î»Î¿á½¶ ÎºÎ¯Î½Î´Ï…Î½Î¿Î¹ ÎºÎ±á½¶ Ï€ÏŒÎ½Î¿Î¹ Ï€Î¿Î»Ï
        
        á½ Ï‡ÏÏŒÎ½Î¿Ï‚ Î³á½°Ï Ï€Î¬Î½Ï„Î± Ï†Î±Î¯Î½ÎµÎ¹ ÎºÎ±á½¶ Î´Î¹Î´Î¬ÏƒÎºÎµÎ¹
        Î¿á½Î´á½²Î½ ÎºÏÏ…Ï€Ï„á½¸Î½ Î¼Î­Î½ÎµÎ¹ á¼Î½ á¼€Î½Î¸ÏÏÏ€Î¿Î¹Ï‚ Î±á¼°ÎµÎ¯
        
        ÏƒÎ¿Ï†Î¯Î± Î´á½² Î¼ÎµÎ³Î¯ÏƒÏ„Î· Ï„á½¸ Î³Î¹Î³Î½ÏÏƒÎºÎµÎ¹Î½ á¼‘Î±Ï…Ï„ÏŒÎ½
        ÎºÎ±á½¶ Ï„á½° Î¸Îµá¿–Î± Î¼á½´ á½‘Ï€ÎµÏÏ†ÏÎ¿Î½Îµá¿–Î½ á¼Î½ Î²ÏÎ¿Ï„Î¿á¿–Ï‚
        """

if __name__ == "__main__":
    print("=" * 60)
    print("CALLIMACHINA STYLOMETRIC ENHANCED ENGINE")
    print("=" * 60)
    
    engine = StylometricEnhanced()
    
    # Test with real fragments
    import yaml
    with open('/Volumes/VIXinSSD/callimachina/pinakes/fragments/enhanced_batch.yml', 'r') as f:
        fragment_data = yaml.safe_load(f)
    
    test_fragments = fragment_data['fragments']
    
    print("\n[ATTRIBUTION ANALYSIS] Real papyrus fragments...")
    results = []
    
    for fragment in test_fragments:
        text = fragment.get('text', '')
        if not text or len(text) < 30:
            continue
            
        attributions = engine.attribute_fragment_robust(text)
        
        if not attributions:
            continue
        
        top_author, composite_score, feature_scores = attributions[0]
        confidence_level, confidence_pct = engine.get_stylometric_confidence(composite_score)
        
        # Get top 3 candidates
        candidates = []
        for author, score, features in attributions[:3]:
            level, pct = engine.get_stylometric_confidence(score)
            candidates.append({
                'author': author,
                'composite_score': score,
                'confidence_level': level,
                'confidence': pct
            })
        
        result = {
            'fragment_id': fragment['id'],
            'top_attribution': top_author,
            'confidence_level': confidence_level,
            'confidence': confidence_pct,
            'composite_score': composite_score,
            'candidates': candidates,
            'feature_breakdown': feature_scores,
            'text_preview': text[:100] + "..." if len(text) > 100 else text,
            'analyzed': datetime.now().isoformat()
        }
        
        results.append(result)
        
        print(f"\nFragment: {fragment['id']}")
        print(f"Text: {text[:60]}...")
        print(f"Attributed to: {top_author} ({confidence_pct:.1%} confidence)")
        print(f"Composite Score: {composite_score:.3f}")
        
        # Issue alert if high confidence
        alert_file = engine.issue_stylometric_alert(result, fragment)
        if alert_file:
            print(f"ğŸš¨ STYLOMETRIC ALERT ISSUED")
    
    # Save comprehensive report
    engine.save_attribution_report(results, '/Volumes/VIXinSSD/callimachina/pinakes/stylometric_analysis.yml')
    
    print(f"\n[STYLOMETRIC ANALYSIS COMPLETE] {len(results)} fragments analyzed")
    print(f"[ALERTS ISSUED] {sum(1 for r in results if r['confidence'] >= 0.70)}")
